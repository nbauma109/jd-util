name: LLM Suggestions PR (Local)

on:
  workflow_dispatch:
    inputs:
      user_prompt:
        description: "Prompt for the LLM (e.g., add docstrings, improve types, small perf fixes)"
        required: false
        type: string
      model:
        description: "Ollama model tag"
        required: false
        default: "llama3.2:3b"
        type: string
      base_branch:
        description: "Base branch for the suggestions PR (defaults to repo default branch)"
        required: false
        type: string
      max_files:
        description: "Max files to consider"
        required: false
        default: "50"
        type: string
      max_file_kb:
        description: "Max size per file (KB)"
        required: false
        default: "120"
        type: string
      max_total_kb:
        description: "Overall context budget (KB)"
        required: false
        default: "800"
        type: string
  schedule:
    - cron: "30 5 * * *"  # daily at 05:30 UTC

permissions:
  contents: write
  pull-requests: write

jobs:
  suggest:
    runs-on: ubuntu-latest
    env:
      LLM_MODEL: ${{ inputs.model }}
      MAX_FILES: ${{ inputs.max_files }}
      MAX_FILE_KB: ${{ inputs.max_file_kb }}
      MAX_TOTAL_KB: ${{ inputs.max_total_kb }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Resolve base branch
        id: base
        shell: bash
        run: |
          set -euo pipefail
          if [ -n "${{ inputs.base_branch }}" ]; then
            base="${{ inputs.base_branch }}"
          else
            base="$(git remote show origin | sed -n '/HEAD branch/s/.*: //p')"
            base="${base:-main}"
          fi
          echo "base=${base}" >> "$GITHUB_OUTPUT"
          echo "Using base branch: ${base}"

      - name: Cache Ollama models
        uses: actions/cache@v4
        with:
          path: ~/.ollama
          key: ${{ runner.os }}-ollama-cache

      - name: Setup Ollama
        uses: ai-action/setup-ollama@v1

      - name: Start Ollama & pull model
        env:
          LLM_MODEL: ${{ env.LLM_MODEL }}
        run: |
          nohup ollama serve > /tmp/ollama.log 2>&1 &
          # wait for daemon to be ready
          for i in {1..30}; do
            if curl -sSf http://localhost:11434/api/version >/dev/null; then break; fi
            sleep 1
          done
          echo "Pulling model: $LLM_MODEL"
          ollama pull "$LLM_MODEL"

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq patch curl
          python -m pip install --upgrade pip
          pip install requests

      - name: Prepare prompt and file list
        id: prep
        env:
          USER_PROMPT: ${{ inputs.user_prompt }}
          BASE_BRANCH: ${{ steps.base.outputs.base }}
          MAX_FILES: ${{ env.MAX_FILES }}
          MAX_FILE_KB: ${{ env.MAX_FILE_KB }}
          MAX_TOTAL_KB: ${{ env.MAX_TOTAL_KB }}
        run: |
          bash .github/scripts/llm_prep.sh
          echo "files_count=$(wc -l < .llm_files.txt | tr -d ' ')" >> "$GITHUB_OUTPUT"

      - name: Generate diff with local LLM
        id: gen
        env:
          LLM_MODEL: ${{ env.LLM_MODEL }}
        run: |
          python .github/scripts/llm_generate.py

      - name: Validate, apply, commit, and open PR
        id: apply
        env:
          LLM_MODEL: ${{ env.LLM_MODEL }}
          BASE_BRANCH: ${{ steps.base.outputs.base }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          bash .github/scripts/llm_validate_apply.sh

      - name: No-op note
        if: steps.apply.outputs.no_changes == 'true'
        run: echo "No changes suggested by the model this run."
